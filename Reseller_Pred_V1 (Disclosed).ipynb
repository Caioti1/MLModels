{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import urllib3\n",
    "from unidecode import unidecode\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from IPython.core.magic import (Magics, magics_class, cell_magic)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "# Pre processing and manipulation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# metrics and hyperparameter optimization\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "from skopt import dummy_minimize\n",
    "from skopt import gp_minimize\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# models\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm, tree\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx_Oracle.init_oracle_client(lib_dir=\"C:\\oracle\\instantclient_19_8\", config_dir=\"C:\\oracle\\instantclient_19_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = \"\"\n",
    "conn = cx_Oracle.connect(\"\", pw, \"\", encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sql_pedidos = '''SELECT V_ORDER_ALL.LOG_SEARCH_ID \"Pesquisa\", V_ORDER_ALL.ID_ORDER \"Pedido\", V_ORDER_ALL.CUSTOMER_ID \"ID Cliente\", V_ORDER_ALL.CUSTOMER \"Cliente\", V_ORDER_ALL.TELEPHONE_CUSTOMER \"Telefone\", V_ORDER_ALL.ORDER_DATE \"Data\", V_ORDER_ALL.MARKET_RESELLER \"Mercado\", V_ORDER_ALL.CORE_RESELLER \"Núcleo\", V_ORDER_ALL.MICRO_MARKET_RESELLER \"Micro Mercado\", V_ORDER_ALL.CNPJ_RESELLER \"CNPJ\", V_ORDER_ALL.COMPANY_NAME_RESELLER \"Nome Fantasia\", V_ORDER_ALL.ORDER_ADDRESS \"Endereço\", V_ORDER_ALL.ORDER_ADDRESS_NUMBER \"Nº\", V_ORDER_ALL.ORDER_ADDRESS_COMPLEMENT \"Complemento\", V_ORDER_ALL.ORDER_ADDRESS_NEIGHBORHOOD \"Bairro\", V_ORDER_ALL.ORDER_ADDRESS_REFERENCE \"Referência\", V_ORDER_ALL.ZIPCODE_CUSTOMER \"CEP\", V_ORDER_ALL.CITY_RESELLER \"Cidade\", V_ORDER_ALL.UF_RESELLER \"UF\", V_ORDER_ALL.CHANNEL \"Canal\", V_ORDER_ALL.ORDER_VOUCHER_PARTNER \"Código Parceiro\", V_ORDER_ALL.SOURCE_ACCESS \"Origem Acesso\", V_ORDER_ALL.SOURCE \"Modo\", V_ORDER_ALL.ORDER_STATUS_DESCR \"Status\", TB_ORDER.DELIVERY_DATE \"Data Entrega\", V_ORDER_ALL.ORDER_RAITING \"Avaliação\", V_ORDER_ALL.PRODUCT \"Produto\", V_ORDER_ALL.PRODUCT_PRICE \"Preço\", V_ORDER_ALL.PRODUCT_QUANTITY \"Quantidade\", V_ORDER_ALL.ORDER_PACKING \"Vasilhame\", V_ORDER_ALL.ORDER_PACKING_COST \"Preço Vasilhame\", V_ORDER_ALL.ORDER_PAYMENT_METHOD \"Forma Pagto\", V_ORDER_ALL.ORDER_TOTAL \"Valor Pedido\", V_ORDER_ALL.ORDER_FEE \"Taxa uso Plataforma (%)\", V_ORDER_ALL.ORDER_FEE_TOTAL \"Taxa uso Plataforma (R$)\", V_ORDER_ALL.ORDER_VOUCHER_CODE \"Vale Gás\", V_ORDER_ALL.ORDER_VOUCHER_TYPE \"Tipo Vale Gás\", V_ORDER_ALL.COUPON \"Cupom\", V_ORDER_ALL.COUPON_PARTNER \"Cupom Parceria\", V_ORDER_ALL.COUPON_PARTNER_NAME \"Cupom Nome Parceiro\", V_ORDER_ALL.COUPON_PARTNER_COST \"Cupom Custo Parceiro\", V_ORDER_ALL.ORDER_SHIP_PROMISSE_TIME \"Tempo Previsto Entrega\", V_ORDER_ALL.NAME_DELIVERYMAN \"Entregador\", V_ORDER_ALL.CANCELLATION_REASON \"Motivo Cancelamento\", V_ORDER_ALL.CANCELLATION_REASON_TYPE \"Justificativa Cancelamento\", V_ORDER_ALL.ORDER_CANCEL_DATE \"Data Cancelamento\", V_ORDER_ALL.ORDER_REDIRECT_RESELLER \"Pedido Remanejado\", V_ORDER_ALL.ORDER_CANCEL_USER \"Usuário Cancelamento\", V_ORDER_ALL.ORDER_ID_NEW_REDIRECT \"Novo Pedido\", V_ORDER_ALL.SESSION_ID \"Sessão\", V_ORDER_ALL.ORDER_CANCEL_SUB_REASON \"Sub Motivo Cancelamento\", V_ORDER_ALL.COMPANY_RESELLER \"Razão Social\", V_ORDER_ALL.ORDER_PAYMENT_ID \"Payment ID\"\n",
    "FROM V_ORDER_ALL\n",
    "    INNER JOIN TB_ORDER\n",
    "    ON V_ORDER_ALL.ID_ORDER = TB_ORDER.ID_ORDER\n",
    "WHERE ORDER_DATE > TO_DATE('2020-01-01','YYYY-MM-DD')\n",
    "AND ORDER_DATE < TO_DATE('2021-01-01','YYYY-MM-DD')\n",
    "'''\n",
    "\n",
    "sql_revendas = '''SELECT TB_RESELLER.CNPJ, V_RESELLER_LIST.LINK_PRODUCT, V_RESELLER_LIST.LINK_AREA, TB_RESELLER.STATUS, TB_RESELLER.LATITUDE, TB_RESELLER.LONGITUDE, TB_RESELLER.INTERNAL_OPERATION \"OP INTERNA\", TB_RESELLER.MARKET_ID, TB_RESELLER.MARKET_CORE_ID, TB_RESELLER.MICRO_MARKET_ID, TB_RESELLER.CITY_ID, TB_RESELLER.CURRENT_FEE, TB_RESELLER.VALIDATE_ONLINE_CODE, TB_RESELLER.PREMIUM_VALUE, TB_RESELLER.RAITING, TB_RESELLER.IS_OPEN,HOUR_START, TB_RESELLER.HOUR_END, TB_RESELLER.HOUR_START_VACANCY, TB_RESELLER.HOUR_END_VACANCY, TB_RESELLER.RECEIVE_SMS, TB_RESELLER.MARKETPLACE, TB_RESELLER.DIRECT_SALE, TB_RESELLER.ENABLE_SCHEDULE  \n",
    "FROM TB_RESELLER\n",
    "    INNER JOIN V_RESELLER_LIST\n",
    "    ON TB_RESELLER.CNPJ = V_RESELLER_LIST.CNPJ\n",
    "'''\n",
    "\n",
    "df_pedidos = pd.read_sql_query(sql_pedidos, conn)\n",
    "df_revendas = pd.read_sql_query(sql_revendas, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime():\n",
    "    df_pedidos['Data'] = pd.to_datetime(df_pedidos['Data'], dayfirst=True)\n",
    "    \n",
    "def new_data_hora():\n",
    "    df_pedidos['Data Ajustada'] = [d.date() for d in df_pedidos['Data']]\n",
    "    df_pedidos['Hora'] = [d.time() for d in df_pedidos['Data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_datetime()\n",
    "new_data_hora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pedidos['Mês'] = pd.DatetimeIndex(df_pedidos['Data Ajustada']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     196\n",
       "7     159\n",
       "8     147\n",
       "2     134\n",
       "6     119\n",
       "1      92\n",
       "4      86\n",
       "5      79\n",
       "3      68\n",
       "12     11\n",
       "11      1\n",
       "10      1\n",
       "Name: Mês, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jon = df_pedidos.groupby(['CNPJ'])\n",
    "jon_mes = jon['Mês'].nunique()\n",
    "df_jon_mes = pd.DataFrame(jon_mes)\n",
    "df_jon_mes['Mês'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d79853a8df2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjon_valor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"Valor Pedido\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_jon_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjon_valor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_jon_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'jon' is not defined"
     ]
    }
   ],
   "source": [
    "jon_valor = jon.aggregate({\"Valor Pedido\":sum})\n",
    "df_jon_val = pd.DataFrame(jon_valor)\n",
    "df_jon_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_jon_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f9d7d0b14df5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#DISCLOSED\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_jon_geral\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_jon_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_jon_mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'CNPJ'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_jon_geral\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Média Mês'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_jon_geral\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Valor Pedido'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdf_jon_geral\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Mês'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_jon_geral\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_jon_val' is not defined"
     ]
    }
   ],
   "source": [
    "#DISCLOSED\n",
    "df_jon_geral = df_jon_val.merge(df_jon_mes, on='CNPJ')\n",
    "df_jon_geral['Média Mês'] = df_jon_geral['Valor Pedido'] / df_jon_geral['Mês']\n",
    "df_jon_geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_revendas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fd05215c2261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#DISCLOSED\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_rev_work\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_revendas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_jon_geral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'CNPJ'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_rev_work\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_revendas' is not defined"
     ]
    }
   ],
   "source": [
    "#DISCLOSED\n",
    "df_rev_work = df_revendas.merge(df_jon_geral, on='CNPJ')\n",
    "df_rev_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CNPJ', 'LINK_PRODUCT', 'LINK_AREA', 'STATUS', 'LATITUDE', 'LONGITUDE',\n",
       "       'OP INTERNA', 'MARKET_ID', 'MARKET_CORE_ID', 'MICRO_MARKET_ID',\n",
       "       'CITY_ID', 'CURRENT_FEE', 'VALIDATE_ONLINE_CODE', 'PREMIUM_VALUE',\n",
       "       'RAITING', 'IS_OPEN', 'HOUR_START', 'HOUR_END', 'HOUR_START_VACANCY',\n",
       "       'HOUR_END_VACANCY', 'RECEIVE_SMS', 'MARKETPLACE', 'DIRECT_SALE',\n",
       "       'ENABLE_SCHEDULE', 'Valor Pedido', 'Mês', 'Média Mês'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev_work.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rev_work2 = df_rev_work.drop(columns=['Nome Fantasia', 'ID', 'Endereço', 'Número', 'Grupos', 'Telefone Celular', 'Telefone Fixo', 'Email', 'Código do Cliente', 'Código do Endereço', 'ID Revenda Backup', 'Nome Revenda Backup', 'Total Pedidos', 'Pedidos Aguardando Pagamento', 'Pedidos Agendado', 'Pedidos Aberto', 'Pedidos Andamento', 'Pedidos Cancelado Revenda', 'Pedidos Cancelado Cliente', 'Pedidos Cancelado Outros', 'Pedidos Entregue', 'Tempo Aceite', 'Tempo Andamento', 'Tempo Entrega', 'Tempo Cancelamento', 'Início Cálculo Performance', 'Data Base', 'Dia do mês para geração faturamento', 'Método de Pagamento Faturamento', 'Condição de Pagamento', 'Taxa Administrador (%)', 'Tipo de Ajuste da Taxa', 'Valor de Tarifa (R$)', 'Tipo de Ajuste da Tarifa', 'ID Braspag', 'Razão Social','Taxa de Serviço',])\n",
    "df_rev_work2 = df_rev_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CNPJ', 'LINK_PRODUCT', 'LINK_AREA', 'STATUS', 'LATITUDE', 'LONGITUDE',\n",
       "       'OP INTERNA', 'MARKET_ID', 'MARKET_CORE_ID', 'MICRO_MARKET_ID',\n",
       "       'CITY_ID', 'CURRENT_FEE', 'VALIDATE_ONLINE_CODE', 'PREMIUM_VALUE',\n",
       "       'RAITING', 'IS_OPEN', 'HOUR_START', 'HOUR_END', 'HOUR_START_VACANCY',\n",
       "       'HOUR_END_VACANCY', 'RECEIVE_SMS', 'MARKETPLACE', 'DIRECT_SALE',\n",
       "       'ENABLE_SCHEDULE', 'Valor Pedido', 'Mês', 'Média Mês'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev_work.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNFORTUNATELY UNABLE TO EXECUTE FROM HERE ON DUE TO ACCESS ISSUES\n",
    "## BUT THE METODOLOGY IS THERE, MANAGED TO GET UP TO 0.72 LAST ROLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jackson(x):\n",
    "    if x < 500:\n",
    "        return 1\n",
    "    elif x < 1000:\n",
    "        return 2\n",
    "    elif x < 2000:\n",
    "        return 3\n",
    "    elif x < 3000:\n",
    "        return 4\n",
    "    elif x < 5000:\n",
    "        return 5\n",
    "    elif x < 10000:\n",
    "        return 6\n",
    "    elif x < 20000:\n",
    "        return 7\n",
    "    else:\n",
    "        return 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_work['class'] = df_rev_work['Média Mês'].apply(jackson)\n",
    "df_rev_work['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_work=df_rev_work[df_rev_work['Média Mês'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rev_local = df_rev_work[['CEP', 'Canais', 'UF', 'Cidade', 'Bairro', 'Média Mês']]\n",
    "df_rev_local = df_rev_work[['CNPJ', 'LINK_PRODUCT', 'LINK_AREA', 'STATUS', 'LATITUDE', 'LONGITUDE', 'OP INTERNA', 'MARKET_ID',\n",
    "       'MARKET_CORE_ID', 'MICRO_MARKET_ID', 'CITY_ID', 'CURRENT_FEE',\n",
    "       'VALIDATE_ONLINE_CODE', 'PREMIUM_VALUE', 'RAITING', 'IS_OPEN',\n",
    "       'HOUR_START', 'HOUR_END', 'RECEIVE_SMS', 'MARKETPLACE', 'DIRECT_SALE', 'ENABLE_SCHEDULE', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df_rev_local.iloc[:,0].values\n",
    "#ohenc = OneHotEncoder()\n",
    "#jaxle = X.reshape(-1, 1)\n",
    "#jaxre = re.fit_transform(jaxle).toarray()\n",
    "#bobby = pd.DataFrame(jaxre)\n",
    "#bobby.columns = re.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jonny = ohenc.fit_transform(df_rev_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jonny = pd.get_dummies(df_rev_local[['STATUS', 'OP INTERNA', 'VALIDATE_ONLINE_CODE', 'IS_OPEN',\n",
    "       'HOUR_START', 'HOUR_END',\n",
    "       'RECEIVE_SMS', 'MARKETPLACE', 'DIRECT_SALE', 'ENABLE_SCHEDULE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jonny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_localxxx = pd.concat([df_rev_local, df_jonny], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_localxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local_final = df_rev_localxxx.drop(columns=['CNPJ', 'STATUS', 'OP INTERNA', 'VALIDATE_ONLINE_CODE', 'IS_OPEN',\n",
    "       'HOUR_START', 'HOUR_END', 'RECEIVE_SMS', 'MARKETPLACE', 'DIRECT_SALE', 'ENABLE_SCHEDULE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local_final['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=pd.Series(df_rev_local_final.columns)\n",
    "for dup in df_rev_local_final.columns[df_rev_local_final.columns.duplicated(keep=False)]: \n",
    "    cols[df_rev_local_final.columns.get_loc(dup)] = ([dup + '.' + str(d_idx) \n",
    "                                     if d_idx != 0 \n",
    "                                     else dup \n",
    "                                     for d_idx in range(df_rev_local_final.columns.get_loc(dup).sum())]\n",
    "                                    )\n",
    "df_rev_local_final.columns=cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local_final2 = df_rev_local_final.loc[:,~df_rev_local_final.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df_rev_local_final2 = df_rev_local_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local_final2 = df_rev_local_final2.loc[:,~df_rev_local_final2.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local_final3 = df_rev_local_final2.dropna(subset=['LATITUDE', 'MARKET_ID'])\n",
    "#df_rev_local_final3 = df_rev_local_final3[df_rev_local_final3['MdiaMs'] <= 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local_final3['PREMIUM_VALUE'] = df_rev_local_final3['PREMIUM_VALUE'].fillna(0)\n",
    "df_rev_local_final3['CURRENT_FEE'] = df_rev_local_final3['CURRENT_FEE'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_local_final3['PREMIUM_VALUE'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_rev_local_final3.drop(columns=['class'])\n",
    "y = df_rev_local_final3['class'] #.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = np.log(y)\n",
    "y_log.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log_train, X_log_test, y_log_train, y_log_test = train_test_split(X, y_log, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log_test.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparando os modelos para verificar qual utilizar\n",
    "#isso demorou MUITO pra rodar\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# criando uma lista com todos os modelos\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(3),\n",
    "    #GaussianNB(),\n",
    "    #LogisticRegression(),\n",
    "    #SVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    LGBMClassifier(),\n",
    "    xgb.XGBClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in classifiers:\n",
    "    # ajustando o modelo\n",
    "    clf.fit(X_train, y_train)\n",
    "    # armazenando o nome do modelo\n",
    "    name = clf.__class__.__name__\n",
    "    # imprimindo o nome do modelo\n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    # imprimindo os resultados\n",
    "    print('****Results****')\n",
    "    # fazendo predições\n",
    "    # calculando as métricas\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # imprimindo as métricas\n",
    "    print(\"Score:\", clf.score(X_test, y_test))\n",
    "    #print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "    #print(\"Recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in classifiers:\n",
    "    # ajustando o modelo\n",
    "    clf.fit(X_log_train, y_log_train)\n",
    "    # armazenando o nome do modelo\n",
    "    name = clf.__class__.__name__\n",
    "    # imprimindo o nome do modelo\n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    # imprimindo os resultados\n",
    "    print('****Results****')\n",
    "    # fazendo predições\n",
    "    # calculando as métricas\n",
    "    y_pred = clf.predict(X_log_test)\n",
    "    # imprimindo as métricas\n",
    "    print(\"Score:\", clf.score(X_log_test, y_log_test))\n",
    "    #print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "    #print(\"Recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUNANDO O GRADIENT BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def treinar_modelo_gradient(params):\n",
    "    #random_state = params[0]\n",
    "    #learning_rate = params[0]\n",
    "    #n_estimators = params[0]\n",
    "    subsample = params[0]\n",
    "    #min_samples_split = params[0]\n",
    "    #max_features = params[0]\n",
    "    #min_weight_fraction_leaf = params[0]\n",
    "    #max_depth = params[0]\n",
    "    \n",
    "    \n",
    "    print(params, '\\n')\n",
    "    \n",
    "    #learning_rate=0.45389327382098293 ou 0.28191499226502426\n",
    "    #n_estimators = 91 ou 97\n",
    "    #subsample = 0.7027664203395149\n",
    "    \n",
    "    mdl_gradient = GradientBoostingClassifier(random_state=43, learning_rate=0.28191499226502426, subsample=subsample)\n",
    "    mdl_gradient.fit(X_train, y_train)\n",
    "    \n",
    "    p2 = mdl_gradient.predict(X_test) #.astype('int32')\n",
    "    \n",
    "    # Queremos minimizar o auc score\n",
    "    return -mdl_gradient.score(X_test, y_test)\n",
    "\n",
    "# Definindo espaço de busca\n",
    "space_gradient = [#(0.01, 0.1)] #learning rate\n",
    "         #(50, 200)] # n_estimators\n",
    "         (0.01, 1)] # subsample\n",
    "         #(2, 30)] # min_sample_split\n",
    "         #(100, 200)] #max_features\n",
    "         #(0.0, 0.05)] #min_weight\n",
    "         #(3, 50)] #max_depth\n",
    "         #(1,50)] # rdm_state\n",
    "\n",
    "# fazendo o fit do modelo com 30 calls\n",
    "resultado_dummy_gradient = dummy_minimize(treinar_modelo_gradient, space_gradient, random_state=1, verbose=1, n_calls=30)\n",
    "resultado_dummy_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultado_gp_gradient = gp_minimize(treinar_modelo_gradient, space_gradient, random_state=1, verbose=1, n_calls=30, n_random_starts=10)\n",
    "resultado_gp_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(resultado_dummy_gradient, resultado_gp_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST MODEL GRADIENT BOOSTING\n",
    "\n",
    "model_GBoost = GradientBoostingRegressor(random_state=1, learning_rate=0.400231532805698, n_estimators=100,\n",
    "                                            subsample=0.8478478075191571, min_samples_split=8, max_depth=3).fit(X_train, y_train)\n",
    "model_GBoost.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUNANDO O LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_modelo_lgbm(params):\n",
    "    #random_state = params[0]\n",
    "    #learning_rate = params[0]\n",
    "    #num_leaves = params[0]\n",
    "    #min_child_samples = params[0]\n",
    "    subsample = params[0]\n",
    "    #colsample_bytree = params[0]\n",
    "    \n",
    "    print(params, '\\n')\n",
    "    \n",
    "    #learning_rate=learning_rate, num_leaves=num_leaves, min_child_samples=min_child_samples,\n",
    "                        #subsample=subsample, colsample_bytree=colsample_bytree, random_state=0, subsample_freq=1, \n",
    "                         #n_estimators=100\n",
    "    \n",
    "    mdl_lgbm = LGBMClassifier(random_state=0, learning_rate=0.6754657915377302, num_leaves=31, \n",
    "                              min_child_samples=20, subsample=subsample)\n",
    "    mdl_lgbm.fit(X_train, y_train)\n",
    "    \n",
    "    p = mdl_lgbm.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Queremos minimizar o auc score\n",
    "    return -mdl_lgbm.score(X_test, y_test)\n",
    "\n",
    "# Definindo nosso espaço de busca randômica. Não são tuplas, são ranges!\n",
    "space_lgbm = [#(1, 60)] #learning rate\n",
    "         #(0.1, 1.0, 'log-uniform')] #learning rate\n",
    "         #(2, 64)] # num_leaves\n",
    "         #(20, 64)] # min_child_samples\n",
    "         (0.05, 1.0)] # subsample\n",
    "         #(0.1, 1.0)] # colsample bytree\n",
    "\n",
    "# fazendo o fit do modelo com 30 calls, ou seja, amostragens\n",
    "resultado = dummy_minimize(treinar_modelo_lgbm, space_lgbm, random_state=1, verbose=1, n_calls=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultado_gp_xgb = gp_minimize(treinar_modelo_lgbm, space_lgbm, random_state=1, verbose=1, n_calls=30, n_random_starts=10)\n",
    "resultado_gp_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MELHOR RESULTADO XGBOOST\n",
    "model_XGBoost = xgb.XGBRegressor(min_child_weight=1, learning_rate=0.28887118156158087, colsample_bytree=0.20921998968669897,\n",
    "                              max_depth=62, subsample=1).fit(X_train, y_train)\n",
    "model_XGBoost.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model_GBoost.predict(X_test)\n",
    "df_pred1 = pd.DataFrame(pred1)\n",
    "df_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred1 = df_pred1.rename(columns={0: \"Valor Predito\"})\n",
    "df_pred1['Valor Real'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ytest = pd.DataFrame(y_test)\n",
    "df_ytest = df_ytest.reset_index()\n",
    "df_ytest = df_ytest.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_ytest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-575fafe0ae9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Valor Real'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_ytest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'MdiaMs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_ytest' is not defined"
     ]
    }
   ],
   "source": [
    "df_pred1['Valor Real'] = df_ytest['MdiaMs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pred1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-373db2d1b118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Diff1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Valor Predito'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Valor Real'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Diff2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Valor Real'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Valor Predito'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_pred1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_pred1' is not defined"
     ]
    }
   ],
   "source": [
    "df_pred1['Diff1'] = df_pred1['Valor Predito']-df_pred1['Valor Real']\n",
    "df_pred1['Diff2'] = df_pred1['Valor Real']-df_pred1['Valor Predito']\n",
    "df_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pred1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-dc78bd187e57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ConfDown'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Diff1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ConfUp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_pred1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Diff1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_pred1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_pred1' is not defined"
     ]
    }
   ],
   "source": [
    "df_pred1['ConfDown'] = -df_pred1['Diff1'].std()*2\n",
    "df_pred1['ConfUp'] = df_pred1['Diff1'].std()*2\n",
    "df_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred1['Diff1'].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred1[['ConfDown','Diff1','ConfUp']].plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred1['Diff1'].plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILIZANDO O DATAFRAME COM TODAS AS VARIÁVEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILIZANDO O DATAFRAME COM TODAS AS VARIÁVEIS\n",
    "df_rev_work2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rev_work2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_full = pd.get_dummies(df_rev_work2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_final = df_encoded_full.drop(columns=['Código IBGE', 'Valor Pedido', 'Mês'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=pd.Series(df_encoded_final.columns)\n",
    "for dup in df_encoded_final.columns[df_encoded_final.columns.duplicated(keep=False)]: \n",
    "    cols[df_encoded_final.columns.get_loc(dup)] = ([dup + '.' + str(d_idx) \n",
    "                                     if d_idx != 0 \n",
    "                                     else dup \n",
    "                                     for d_idx in range(df_encoded_final.columns.get_loc(dup).sum())]\n",
    "                                    )\n",
    "df_encoded_final.columns=cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_final2 = df_encoded_final.loc[:,~df_encoded_final.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_final2 = df_encoded_final2.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_final2 = df_encoded_final2.loc[:,~df_encoded_final2.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_final2 = df_encoded_final2[df_encoded_final2['MdiaMs'] != 0]\n",
    "df_encoded_final2['MdiaMs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = df_encoded_final2.drop(columns=['MdiaMs'])\n",
    "y_full = df_encoded_final2['MdiaMs'] #.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full_log = np.log(y_full)\n",
    "y_full_log.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_train, X_full_test, y_full_train, y_full_test = train_test_split(X_full, y_full, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_train, X_full_test, y_full_log_train, y_full_log_test = train_test_split(X, y_full_log, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparando os modelos para verificar qual utilizar\n",
    "#isso demorou MUITO pra rodar\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "\n",
    "# criando uma lista com todos os modelos\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(3),\n",
    "    #GaussianNB(),\n",
    "    #LogisticRegression(),\n",
    "    #SVC(),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    LGBMRegressor(),\n",
    "    xgb.XGBRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in classifiers:\n",
    "    # ajustando o modelo\n",
    "    clf.fit(X_full_train, y_full_train)\n",
    "    # armazenando o nome do modelo\n",
    "    name = clf.__class__.__name__\n",
    "    # imprimindo o nome do modelo\n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    # imprimindo os resultados\n",
    "    print('****Results****')\n",
    "    # fazendo predições\n",
    "    # calculando as métricas\n",
    "    y_pred = clf.predict(X_full_test)\n",
    "    # imprimindo as métricas\n",
    "    print(\"Score:\", clf.score(X_full_test, y_full_test))\n",
    "    #print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "    #print(\"Recall:\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_modelo_xgb(params):\n",
    "    random_state = params[0]\n",
    "    #eta = params[0]\n",
    "    #min_child_weight = params[0]\n",
    "    #max_depth = params[0]\n",
    "    #max_leaves = params[0]\n",
    "    #gamma = params[4]\n",
    "    #subsample = params[5]\n",
    "    #colsample_bytree = params[6]\n",
    "    \n",
    "    \n",
    "    print(params, '\\n')\n",
    "    \n",
    "    #learning_rate=lrn_rate, n_estimators=num_estim, subsample=subsample, min_samples_split=min_samp_split, \n",
    "    #                                     min_samples_leaf=min_samp_leaf, min_weight_fraction_leaf=min_weight, max_depth=max_depth\n",
    "    \n",
    "    mdl_xgb = GradientBoostingRegressor(random_state=15, )\n",
    "    mdl_xgb.fit(X_full_train, y_full_train)\n",
    "    \n",
    "    p2_xgb = mdl_xgb.predict(X_full_test) #.astype('int32')\n",
    "    \n",
    "    # Queremos minimizar o auc score\n",
    "    return -mdl_xgb.score(X_full_test, y_full_test)\n",
    "\n",
    "# Definindo espaço de busca\n",
    "space = [(1, 50)] #random_state\n",
    "         #(0.01, 0.5)] #eta\n",
    "         #(0, 1)] # min_child_weight\n",
    "         #(1, 6)] # max_depth\n",
    "         #(50, 150)] # max_leaves\n",
    "         #(0,5), #gamma\n",
    "         #(0, 1), #subsample\n",
    "         #(0, 1)] #colsample_bytree\n",
    "\n",
    "# fazendo o fit do modelo com 30 calls\n",
    "resultado_dummy_xgb = dummy_minimize(treinar_modelo_xgb, space, random_state=1, verbose=1, n_calls=30)\n",
    "resultado_dummy_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_xgb.predict(X_test)\n",
    "\n",
    "x_ax = range(len(y_full_test))\n",
    "plt.plot(x_ax, y_full_test, label=\"original\")\n",
    "plt.plot(x_ax, y_full_pred, label=\"predicted\")\n",
    "plt.title(\"Boston test and predicted data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(resultado_dummy_xgb, resultado_gp_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def treinar_modelo_gradient_full(params):\n",
    "    #random_state = params[0]\n",
    "    #learning_rate = params[0]\n",
    "    n_estimators = params[0]\n",
    "    #subsample = params[0]\n",
    "    #min_samples_split = params[0]\n",
    "    #max_features = params[0]\n",
    "    #min_weight_fraction_leaf = params[0]\n",
    "    #max_depth = params[0]\n",
    "    \n",
    "    \n",
    "    print(params, '\\n')\n",
    "    \n",
    "    #learning_rate=lrn_rate, n_estimators=num_estim, subsample=subsample, min_samples_split=min_samp_split, \n",
    "    #                                     min_samples_leaf=min_samp_leaf, min_weight_fraction_leaf=min_weight, max_depth=max_depth\n",
    "    \n",
    "    mdl_gradient_full = GradientBoostingRegressor(random_state=15, learning_rate=0.27260830969150496, \n",
    "                                                  n_estimators=n_estimators)\n",
    "    mdl_gradient_full.fit(X_full_train, y_full_train)\n",
    "    \n",
    "    p2 = mdl_gradient_full.predict(X_full_test) #.astype('int32')\n",
    "    \n",
    "    # Queremos minimizar o auc score\n",
    "    return -mdl_gradient_full.score(X_full_test, y_full_test)\n",
    "\n",
    "# Definindo espaço de busca\n",
    "space_gradient_full = [#(0.09, 0.3)] #learning rate\n",
    "         (50, 500)] # n_estimators\n",
    "         #(0.01, 0.9)] # subsample\n",
    "         #(2, 50)] # min_sample_split\n",
    "         #(1, 1020, 10)] #max_features\n",
    "         #(0, 0.3)] #min_weight\n",
    "         #(20, 100)] #max_depth\n",
    "         #(1,50)] # rdm_state\n",
    "\n",
    "# fazendo o fit do modelo com 30 calls\n",
    "resultado_dummy_gradient_full = dummy_minimize(treinar_modelo_gradient_full, space_gradient_full, random_state=1, verbose=1, n_calls=30)\n",
    "resultado_dummy_gradient_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_gp = gp_minimize(treinar_modelo_gradient_full, space_gradient_full, random_state=1, verbose=1, n_calls=30, n_random_starts=10)\n",
    "resultado_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(resultado_dummy, resultado_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = mdl.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in mdl.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
